{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 5: Evaluation metrics\n",
    "### Associated lectures: [Lectures 9, 10](https://ubc-cs.github.io/cpsc330/README.html) \n",
    "\n",
    "**Due date: Monday, Feb 27, 2023 at 11:59pm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "from hashlib import sha1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions \n",
    "<hr>\n",
    "rubric={points:3}\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2022W2/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work with a partner on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 2. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Precision, recall, and f1 score by hand <a name=\"1\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the problem of predicting whether a patient has cancer or not. It is important to catch this disease early to reduce mortality rate; late diagnosis will result in metastasis to other organs, which adversely impacts patient's prognosis. Below are confusion matrices of two machine learning models: Model A and Model B. \n",
    "\n",
    "- Model A\n",
    "\n",
    "|         | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 48 | 32 |\n",
    "| **Actual no disease**       | 20 | 100 |\n",
    "\n",
    "\n",
    "- Model B\n",
    "\n",
    "|        | Predicted disease | Predicted no disease |\n",
    "| :------------- | -----------------------: | -----------------------: |\n",
    "| **Actual disease**       | 43 | 22 |\n",
    "| **Actual no disease**       | 35 | 100 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Positive vs. negative class \n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Precision, recall, and f1 score depend upon which class is considered \"positive\", that is the thing you wish to find. In the example above, which class is likely to be the \"positive\" class? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class that is likely to serve as the \"positive\" class could be the disease class. The \"positive\" class can be thought of as the class we wish the model to detect. In cases of predicting health conditions, the disease present class is often chosen as the \"positive\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Accuracy\n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Calculate accuracies for Model A and Model B. \n",
    "\n",
    "We'll store all metrics associated with Model A and Model B in the `results_dict` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {\"A\": {}, \"B\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"A\"][\"accuracy\"] = (100 + 48) / (100 + 48 + 32 + 20)\n",
    "results_dict[\"B\"][\"accuracy\"] = (100 + 43) / (100 + 43 + 22 + 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Which model would you pick? \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Which model would you pick simply based on the accuracy metric? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.74\n",
      "0.715\n"
     ]
    }
   ],
   "source": [
    "print(results_dict[\"A\"][\"accuracy\"])\n",
    "print(results_dict[\"B\"][\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solely based on the accuracy scores above, model A has a higher calculated accuracy for our predictions and thus would be considered the better model. This does not take into account that since a late diagnosis is more costly to our situation, we would likely select the model which predicts accurately while minimizing the amount of false positives (predicting no disease when there really is disease). In this case, we would select Model B as it has a smaller rate of false positives than model A. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Precision, recall, f1-score\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate precision, recall, f1-score for Model A and Model B manually, without using `scikit-learn` tools. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"A\"][\"precision\"] = (48)/(48+20)\n",
    "results_dict[\"B\"][\"precision\"] = (43)/(43+35)\n",
    "results_dict[\"A\"][\"recall\"] = (48)/(48+32)\n",
    "results_dict[\"B\"][\"recall\"] = (43)/(43+22)\n",
    "results_dict[\"A\"][\"f1\"] = (2)*((results_dict[\"A\"][\"precision\"] * results_dict[\"A\"][\"recall\"])/(results_dict[\"A\"][\"precision\"] + results_dict[\"A\"][\"recall\"]))\n",
    "results_dict[\"B\"][\"f1\"] = (2)*((results_dict[\"B\"][\"precision\"] * results_dict[\"B\"][\"recall\"])/(results_dict[\"B\"][\"precision\"] + results_dict[\"B\"][\"recall\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dataframe with all results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.715000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.551282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.661538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.601399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  A         B\n",
       "accuracy   0.740000  0.715000\n",
       "precision  0.705882  0.551282\n",
       "recall     0.600000  0.661538\n",
       "f1         0.648649  0.601399"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Discussion\n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Given the type of problem (early cancer diagnosis), which metric is more informative in this problem? Why? \n",
    "2. Which model would you pick based on this information? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given that we want to reduce late cancer diagnosis, we should reduce the rate of false negative errors. This would mean that recall is more informative as is it directly depends on the rate of false negatives, with higher recall indicating a reduced rate of false negatives and vice versa. \n",
    "\n",
    "2. Model B in this case would be considered the better model as it has a higher recall score than model A, indicating that model B produces less false negative errors than model A, allowing us to optimize for early cancer diagnosis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 1.6 \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Provide 2 to 3 example classification datasets (with links) where accuracy metric would be misleading. Discuss which evaluation metric would be more appropriate for each dataset. You may consider datasets we have used in this course so far. You could also look up datasets on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Credit card fraud detection dataset. Accuracy metric misleading due to extremely imbalanced classes. Minimizing recall would be the better strategy to ensure as many positive cases as positive are caught. https://www.kaggle.com/mlg-ulb/creditcardfraud.\n",
    "\n",
    "2. Breast Cancer detection dataset. Moderate class imbalance makes accuracy metric misleading. Minimizing precision to reduce false positive errors and consequently unecessary invasive treatment would be the better strategy. https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic).\n",
    "\n",
    "3. Adult Census Income Dataset. Although minimal class imbalance, the real life cost of predicting false positive or false negative errors is not equal. In this case, optimizing either precision or recall would be a better strategy. https://archive.ics.uci.edu/ml/datasets/Adult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Classification evaluation metrics using `sklearn` <a name=\"2\"></a>\n",
    "<hr>\n",
    "\n",
    "In general, when a dataset is imbalanced, accuracy does not provide the whole story. In class, we looked at credit card fraud dataset which is a classic example of an imbalanced dataset. \n",
    "\n",
    "Another example is customer churn datasets. [Customer churn](https://en.wikipedia.org/wiki/Customer_attrition) refers to the notion of customers leaving a subscription service like Netflix. In this exercise, we will try to predict customer churn in a dataset where most of the customers stay with the service and a small minority cancel their subscription. To start, please download the [Kaggle telecom customer churn dataset](https://www.kaggle.com/becksddf/churn-in-telecoms-dataset). Once you have the data, you should be able to run the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The starter code below reads the data CSV as a pandas dataframe and splits it into 70% train and 30% test. \n",
    "\n",
    "Note that `churn` column in the dataset is the target. \"True\" means the customer left the subscription (churned) and \"False\" means they stayed.\n",
    "\n",
    "> Note that for this kind of problem a more appropriate technique is something called survival analysis and we'll be talking about it later in the course. For now, we'll just treat it as a binary classification problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>account length</th>\n",
       "      <th>area code</th>\n",
       "      <th>phone number</th>\n",
       "      <th>international plan</th>\n",
       "      <th>voice mail plan</th>\n",
       "      <th>number vmail messages</th>\n",
       "      <th>total day minutes</th>\n",
       "      <th>total day calls</th>\n",
       "      <th>total day charge</th>\n",
       "      <th>...</th>\n",
       "      <th>total eve calls</th>\n",
       "      <th>total eve charge</th>\n",
       "      <th>total night minutes</th>\n",
       "      <th>total night calls</th>\n",
       "      <th>total night charge</th>\n",
       "      <th>total intl minutes</th>\n",
       "      <th>total intl calls</th>\n",
       "      <th>total intl charge</th>\n",
       "      <th>customer service calls</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>NE</td>\n",
       "      <td>70</td>\n",
       "      <td>415</td>\n",
       "      <td>421-8535</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>213.4</td>\n",
       "      <td>86</td>\n",
       "      <td>36.28</td>\n",
       "      <td>...</td>\n",
       "      <td>77</td>\n",
       "      <td>17.40</td>\n",
       "      <td>256.6</td>\n",
       "      <td>101</td>\n",
       "      <td>11.55</td>\n",
       "      <td>5.7</td>\n",
       "      <td>4</td>\n",
       "      <td>1.54</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1855</th>\n",
       "      <td>WI</td>\n",
       "      <td>67</td>\n",
       "      <td>510</td>\n",
       "      <td>417-2265</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>109.1</td>\n",
       "      <td>134</td>\n",
       "      <td>18.55</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>12.10</td>\n",
       "      <td>91.2</td>\n",
       "      <td>86</td>\n",
       "      <td>4.10</td>\n",
       "      <td>10.9</td>\n",
       "      <td>5</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>NJ</td>\n",
       "      <td>122</td>\n",
       "      <td>415</td>\n",
       "      <td>327-9341</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>34</td>\n",
       "      <td>146.4</td>\n",
       "      <td>104</td>\n",
       "      <td>24.89</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>7.62</td>\n",
       "      <td>220.0</td>\n",
       "      <td>91</td>\n",
       "      <td>9.90</td>\n",
       "      <td>15.6</td>\n",
       "      <td>4</td>\n",
       "      <td>4.21</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>NV</td>\n",
       "      <td>107</td>\n",
       "      <td>510</td>\n",
       "      <td>419-9688</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>234.1</td>\n",
       "      <td>91</td>\n",
       "      <td>39.80</td>\n",
       "      <td>...</td>\n",
       "      <td>105</td>\n",
       "      <td>13.86</td>\n",
       "      <td>282.5</td>\n",
       "      <td>100</td>\n",
       "      <td>12.71</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.70</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>HI</td>\n",
       "      <td>105</td>\n",
       "      <td>510</td>\n",
       "      <td>364-8128</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>125.4</td>\n",
       "      <td>116</td>\n",
       "      <td>21.32</td>\n",
       "      <td>...</td>\n",
       "      <td>95</td>\n",
       "      <td>22.23</td>\n",
       "      <td>241.6</td>\n",
       "      <td>104</td>\n",
       "      <td>10.87</td>\n",
       "      <td>11.4</td>\n",
       "      <td>9</td>\n",
       "      <td>3.08</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2154</th>\n",
       "      <td>WY</td>\n",
       "      <td>126</td>\n",
       "      <td>408</td>\n",
       "      <td>339-9798</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>197.6</td>\n",
       "      <td>126</td>\n",
       "      <td>33.59</td>\n",
       "      <td>...</td>\n",
       "      <td>112</td>\n",
       "      <td>20.95</td>\n",
       "      <td>285.3</td>\n",
       "      <td>104</td>\n",
       "      <td>12.84</td>\n",
       "      <td>12.5</td>\n",
       "      <td>8</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>WV</td>\n",
       "      <td>70</td>\n",
       "      <td>510</td>\n",
       "      <td>348-3777</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>30</td>\n",
       "      <td>143.4</td>\n",
       "      <td>72</td>\n",
       "      <td>24.38</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>14.45</td>\n",
       "      <td>127.9</td>\n",
       "      <td>68</td>\n",
       "      <td>5.76</td>\n",
       "      <td>9.4</td>\n",
       "      <td>4</td>\n",
       "      <td>2.54</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1766</th>\n",
       "      <td>NJ</td>\n",
       "      <td>125</td>\n",
       "      <td>415</td>\n",
       "      <td>406-6400</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>182.3</td>\n",
       "      <td>64</td>\n",
       "      <td>30.99</td>\n",
       "      <td>...</td>\n",
       "      <td>121</td>\n",
       "      <td>11.88</td>\n",
       "      <td>171.6</td>\n",
       "      <td>96</td>\n",
       "      <td>7.72</td>\n",
       "      <td>11.6</td>\n",
       "      <td>7</td>\n",
       "      <td>3.13</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>NE</td>\n",
       "      <td>159</td>\n",
       "      <td>415</td>\n",
       "      <td>362-5111</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>189.1</td>\n",
       "      <td>105</td>\n",
       "      <td>32.15</td>\n",
       "      <td>...</td>\n",
       "      <td>147</td>\n",
       "      <td>20.92</td>\n",
       "      <td>242.0</td>\n",
       "      <td>106</td>\n",
       "      <td>10.89</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "      <td>2.81</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1346</th>\n",
       "      <td>PA</td>\n",
       "      <td>106</td>\n",
       "      <td>408</td>\n",
       "      <td>403-9167</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>133.7</td>\n",
       "      <td>45</td>\n",
       "      <td>22.73</td>\n",
       "      <td>...</td>\n",
       "      <td>107</td>\n",
       "      <td>15.96</td>\n",
       "      <td>181.9</td>\n",
       "      <td>89</td>\n",
       "      <td>8.19</td>\n",
       "      <td>10.7</td>\n",
       "      <td>2</td>\n",
       "      <td>2.89</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2333 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     state  account length  area code phone number international plan  \\\n",
       "1402    NE              70        415     421-8535                 no   \n",
       "1855    WI              67        510     417-2265                 no   \n",
       "633     NJ             122        415     327-9341                 no   \n",
       "1483    NV             107        510     419-9688                yes   \n",
       "2638    HI             105        510     364-8128                 no   \n",
       "...    ...             ...        ...          ...                ...   \n",
       "2154    WY             126        408     339-9798                yes   \n",
       "3089    WV              70        510     348-3777                 no   \n",
       "1766    NJ             125        415     406-6400                 no   \n",
       "1122    NE             159        415     362-5111                 no   \n",
       "1346    PA             106        408     403-9167                yes   \n",
       "\n",
       "     voice mail plan  number vmail messages  total day minutes  \\\n",
       "1402              no                      0              213.4   \n",
       "1855              no                      0              109.1   \n",
       "633              yes                     34              146.4   \n",
       "1483              no                      0              234.1   \n",
       "2638              no                      0              125.4   \n",
       "...              ...                    ...                ...   \n",
       "2154              no                      0              197.6   \n",
       "3089             yes                     30              143.4   \n",
       "1766              no                      0              182.3   \n",
       "1122              no                      0              189.1   \n",
       "1346              no                      0              133.7   \n",
       "\n",
       "      total day calls  total day charge  ...  total eve calls  \\\n",
       "1402               86             36.28  ...               77   \n",
       "1855              134             18.55  ...               76   \n",
       "633               104             24.89  ...              103   \n",
       "1483               91             39.80  ...              105   \n",
       "2638              116             21.32  ...               95   \n",
       "...               ...               ...  ...              ...   \n",
       "2154              126             33.59  ...              112   \n",
       "3089               72             24.38  ...               92   \n",
       "1766               64             30.99  ...              121   \n",
       "1122              105             32.15  ...              147   \n",
       "1346               45             22.73  ...              107   \n",
       "\n",
       "      total eve charge  total night minutes  total night calls  \\\n",
       "1402             17.40                256.6                101   \n",
       "1855             12.10                 91.2                 86   \n",
       "633               7.62                220.0                 91   \n",
       "1483             13.86                282.5                100   \n",
       "2638             22.23                241.6                104   \n",
       "...                ...                  ...                ...   \n",
       "2154             20.95                285.3                104   \n",
       "3089             14.45                127.9                 68   \n",
       "1766             11.88                171.6                 96   \n",
       "1122             20.92                242.0                106   \n",
       "1346             15.96                181.9                 89   \n",
       "\n",
       "      total night charge  total intl minutes  total intl calls  \\\n",
       "1402               11.55                 5.7                 4   \n",
       "1855                4.10                10.9                 5   \n",
       "633                 9.90                15.6                 4   \n",
       "1483               12.71                10.0                 3   \n",
       "2638               10.87                11.4                 9   \n",
       "...                  ...                 ...               ...   \n",
       "2154               12.84                12.5                 8   \n",
       "3089                5.76                 9.4                 4   \n",
       "1766                7.72                11.6                 7   \n",
       "1122               10.89                10.4                 5   \n",
       "1346                8.19                10.7                 2   \n",
       "\n",
       "      total intl charge  customer service calls  churn  \n",
       "1402               1.54                       1  False  \n",
       "1855               2.94                       2  False  \n",
       "633                4.21                       2  False  \n",
       "1483               2.70                       1  False  \n",
       "2638               3.08                       2  False  \n",
       "...                 ...                     ...    ...  \n",
       "2154               3.38                       2  False  \n",
       "3089               2.54                       3  False  \n",
       "1766               3.13                       2  False  \n",
       "1122               2.81                       1   True  \n",
       "1346               2.89                       1   True  \n",
       "\n",
       "[2333 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bigml_59c28831336c6604c800002a.csv\", encoding=\"utf-8\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=123)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Distribution of target values\n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Examine the distribution of target values in the train split. Do you see class imbalance? If yes, do we need to deal with it? Why or why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    1984\n",
      "True      349\n",
      "Name: churn, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# count the number of True and False values in 'col1'\n",
    "counts = train_df['churn'].value_counts()\n",
    "\n",
    "# print the counts\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the information from value_counts(), we can see that there is a large class imbalance in favor of false in the target values. In some cases, as long as the model performance during prediction is acceptable, we can ignore this class imbalance. However in this case, we should deal with the class imbalance as the minority class \"true\" is of particular interest when it comes to prediction as we want to accurately determine how many customers have left or stayed in the subscription. This means that we should perform some sort of processing to deal with the imbalance, for example weighting or changing the data counts directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) 2.2 EDA \n",
    "rubric={points:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Come up with **two** exploratory questions you would like to answer and explore those. Briefly discuss your results in 1-3 sentences.\n",
    "\n",
    "You are welcome to use `pandas_profiling` (see Lecture 10) but you don't have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Column transformer \n",
    "rubric={points:14}\n",
    "\n",
    "The code below creates `X_train`, `y_train`, `X_test`, `y_test` for you. \n",
    "In preparation for building a classifier, set up a `ColumnTransformer` that performs whatever feature transformations you deem sensible. This can include dropping features if you think they are not helpful. Remember that by default `ColumnTransformer` will drop any columns that aren't accounted for when it's created.\n",
    "\n",
    "For each group of features (e.g. numeric, categorical or else) explain why you are applying the particular transformation. For example, \"I am doing transformation X to the following categorical features: `a`, `b`, `c` because of reason Y,\" etc.\n",
    "\n",
    "Finally, fit `ColumnTransformer` on your training set; and use the `ColumnTransformer` to transform your train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"churn\"])\n",
    "X_test = test_df.drop(columns=[\"churn\"])\n",
    "\n",
    "y_train = train_df[\"churn\"]\n",
    "y_test = test_df[\"churn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2333 entries, 1402 to 1346\n",
      "Data columns (total 21 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   state                   2333 non-null   object \n",
      " 1   account length          2333 non-null   int64  \n",
      " 2   area code               2333 non-null   int64  \n",
      " 3   phone number            2333 non-null   object \n",
      " 4   international plan      2333 non-null   object \n",
      " 5   voice mail plan         2333 non-null   object \n",
      " 6   number vmail messages   2333 non-null   int64  \n",
      " 7   total day minutes       2333 non-null   float64\n",
      " 8   total day calls         2333 non-null   int64  \n",
      " 9   total day charge        2333 non-null   float64\n",
      " 10  total eve minutes       2333 non-null   float64\n",
      " 11  total eve calls         2333 non-null   int64  \n",
      " 12  total eve charge        2333 non-null   float64\n",
      " 13  total night minutes     2333 non-null   float64\n",
      " 14  total night calls       2333 non-null   int64  \n",
      " 15  total night charge      2333 non-null   float64\n",
      " 16  total intl minutes      2333 non-null   float64\n",
      " 17  total intl calls        2333 non-null   int64  \n",
      " 18  total intl charge       2333 non-null   float64\n",
      " 19  customer service calls  2333 non-null   int64  \n",
      " 20  churn                   2333 non-null   bool   \n",
      "dtypes: bool(1), float64(8), int64(8), object(4)\n",
      "memory usage: 385.0+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations:\n",
    "\n",
    "Categorical:\n",
    "\n",
    "State: OHE\n",
    "\n",
    "International Plan: OHE\n",
    "\n",
    "Voice mail plan: OHE\n",
    "\n",
    "Churn: OHE\n",
    "\n",
    "area code: OHE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Numeric:\n",
    "\n",
    "Account Length: scale\n",
    "\n",
    "phone number: ? drop\n",
    "\n",
    "number vmail messages: scale\n",
    "\n",
    "total day minutes/calls/charge: scale\n",
    "\n",
    "total eve minutes/calls/charge: scale\n",
    "\n",
    "total night minutes/calls/charge: scale\n",
    "\n",
    "total intl minutues/calls/charge: scale\n",
    "\n",
    "customer service calls: scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be applying standard scaling to the numeric features because this allows the model to optimize without data points being too far or close together, while also preventing getting caught in local minimas.\n",
    "The numeric features are 'account length', 'number vmail messages', 'total day minutes', 'total day calls', \n",
    "                    'total day charge', 'total eve minutes', 'total eve calls', 'total eve charge',\n",
    "                    'total night minutes', 'total night calls', 'total night charge', 'total intl minutes',\n",
    "                    'total intl calls', 'total intl charge', 'customer service calls'. \n",
    "\n",
    "I will be applying OHE to categorical features as this converts data into numerical format. 'state', 'area code', 'international plan', 'voice mail plan'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"account length\", \"number vmail messages\", \"total day calls\",\"total day minutes\", \"total day charge\", \"total eve calls\",\"total eve minutes\", \"total eve charge\", \"total night calls\",\"total night minutes\", \"total night charge\",\"total intl calls\",\"total intl minutes\", \"total intl charge\", \"customer service calls\"]\n",
    "categorical_features = [\"state\", \"international plan\", \"voice mail plan\", \"area code\"]\n",
    "drop_features = [\"phone number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lecture 11\n",
    "\n",
    "numeric_transformer = make_pipeline(StandardScaler())\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = preprocessor.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 area code feature\n",
    "rubric={points:4}\n",
    "\n",
    "The original dataset had a feature called `area code`.\n",
    "\n",
    "1. The area codes are numbers. Does it make sense to encode them as one-hot-endoded (OHE) or not? Please justify your response.\n",
    "2. What were the possible values of `area code`? \n",
    "3. If area code is encoded with OHE, how many new features are created to replace it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "415    1178\n",
      "408     588\n",
      "510     567\n",
      "Name: area code, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = train_df['area code'].value_counts()\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "1. Yes, OHE would be preferred in this case as area code is the direct numeric representation of different area codes. This would allow for the encoding to be reduced to smaller integers which is a more interpretable format and convenient in our case since there is only 3 possible area codes. \n",
    "\n",
    "2. What were the possible values of `area code`?  \n",
    "\n",
    "    415, 408, 510\n",
    "\n",
    "\n",
    "3. \n",
    "Only three new features would need to be created to replace the area code feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Logistic regression\n",
    "rubric={points:12} \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Report the cross-validation results of a `LogisticRegression` model, with default Hparams, on the following metrics: `\"accuracy\", \"precision\", \"recall\", \"f1\"`\n",
    "2. Are you satisfied with the results? Explain why or why not. Discuss in a few sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.116649</td>\n",
       "      <td>0.011915</td>\n",
       "      <td>0.867238</td>\n",
       "      <td>0.864416</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.335958</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.229391</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.627451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047514</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>0.854390</td>\n",
       "      <td>0.868703</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.370180</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.654545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047517</td>\n",
       "      <td>0.010524</td>\n",
       "      <td>0.850107</td>\n",
       "      <td>0.868167</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.372449</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.646018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.061519</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.869099</td>\n",
       "      <td>0.866095</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.362245</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.253571</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.633929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.043181</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.836910</td>\n",
       "      <td>0.869845</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.265233</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.660714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  train_accuracy   test_f1  train_f1  \\\n",
       "0  0.116649    0.011915       0.867238        0.864416  0.367347  0.335958   \n",
       "1  0.047514    0.010458       0.854390        0.868703  0.291667  0.370180   \n",
       "2  0.047517    0.010524       0.850107        0.868167  0.255319  0.372449   \n",
       "3  0.061519    0.010501       0.869099        0.866095  0.371134  0.362245   \n",
       "4  0.043181    0.010450       0.836910        0.869845  0.240000  0.378517   \n",
       "\n",
       "   test_recall  train_recall  test_precision  train_precision  \n",
       "0     0.257143      0.229391        0.642857         0.627451  \n",
       "1     0.200000      0.258065        0.538462         0.654545  \n",
       "2     0.171429      0.261649        0.500000         0.646018  \n",
       "3     0.260870      0.253571        0.642857         0.633929  \n",
       "4     0.171429      0.265233        0.400000         0.660714  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from chapter 9\n",
    "scoring = [\n",
    "    \"accuracy\",\n",
    "    \"f1\",\n",
    "    \"recall\",\n",
    "    \"precision\",\n",
    "]\n",
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression())\n",
    "\n",
    "scores = cross_validate(\n",
    "    pipe_lr, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. No. Although fitting and scoring times are not unreasonable and there should be very little overfitting occurring. Precision is relatively low meaning that of the predicted positive examples, many were false positive errors. Recall being low as well means that of all positive examples, only about 23-27% were correctly predicted in the training set, with a lower range present in the test set. Both these metrics are combined in an F1 score that is relatively low for both the training and test sets. These metrics suggest that our model is not preforming well in correctly predicting positive examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Logistic regression with `class_weight`\n",
    "rubric={points:6}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Set the `class_weight` parameter of your logistic regression model to `'balanced'` and report the same metrics as in the previous part. \n",
    "2. Do you prefer this model to the one in the previous part? Discuss your results in a few sentences while comparing the metrics of this model and the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.100207</td>\n",
       "      <td>0.012560</td>\n",
       "      <td>0.785867</td>\n",
       "      <td>0.769025</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.497083</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.763441</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.368512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.045584</td>\n",
       "      <td>0.010819</td>\n",
       "      <td>0.770878</td>\n",
       "      <td>0.769025</td>\n",
       "      <td>0.492891</td>\n",
       "      <td>0.499419</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.770609</td>\n",
       "      <td>0.368794</td>\n",
       "      <td>0.369416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.062248</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.764454</td>\n",
       "      <td>0.773848</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.510441</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.788530</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.377358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049475</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>0.751073</td>\n",
       "      <td>0.780932</td>\n",
       "      <td>0.462963</td>\n",
       "      <td>0.520516</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.792857</td>\n",
       "      <td>0.340136</td>\n",
       "      <td>0.387435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055627</td>\n",
       "      <td>0.010628</td>\n",
       "      <td>0.731760</td>\n",
       "      <td>0.785753</td>\n",
       "      <td>0.434389</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.317881</td>\n",
       "      <td>0.394046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_accuracy  train_accuracy   test_f1  train_f1  \\\n",
       "0  0.100207    0.012560       0.785867        0.769025  0.489796  0.497083   \n",
       "1  0.045584    0.010819       0.770878        0.769025  0.492891  0.499419   \n",
       "2  0.062248    0.010662       0.764454        0.773848  0.455446  0.510441   \n",
       "3  0.049475    0.010198       0.751073        0.780932  0.462963  0.520516   \n",
       "4  0.055627    0.010628       0.731760        0.785753  0.434389  0.529412   \n",
       "\n",
       "   test_recall  train_recall  test_precision  train_precision  \n",
       "0     0.685714      0.763441        0.380952         0.368512  \n",
       "1     0.742857      0.770609        0.368794         0.369416  \n",
       "2     0.657143      0.788530        0.348485         0.377358  \n",
       "3     0.724638      0.792857        0.340136         0.387435  \n",
       "4     0.685714      0.806452        0.317881         0.394046  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from chapter 9\n",
    "\n",
    "pipe_lr_balanced = make_pipeline(preprocessor, LogisticRegression(max_iter = 1000, class_weight = 'balanced'))\n",
    "\n",
    "scores_balanced = cross_validate(\n",
    "    pipe_lr_balanced, X_train, y_train, return_train_score=True, scoring=scoring\n",
    ")\n",
    "\n",
    "pd.DataFrame(scores_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Yes, the improved recall scores and associated f1 score increase suggest that addressing the class imbalance in the data was able to improve the predictive performance of our model. However, precision scores on both the test and training set decreased compared to the unbalanced Logistical Regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameter optimization\n",
    "rubric={points:10}\n",
    "\n",
    "1. Jointly optimize `C` and `class_weight` with `GridSearchCV` and `scoring=\"f1\"`.\n",
    "  - For `class_weight`, consider 3 values: \n",
    "    - `None` (no weight)\n",
    "    - \"weight of class 0 = 1\"  and  \"weight of class 1 = 3\"\n",
    "    - '`balanced`'\n",
    "  - For `C`, choose some reasonable values\n",
    "2. What values of `C` and `class_weight` are chosen and what is the best cross-validation f1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'logisticregression__class_weight': [None, {0:1, 1:3}, 'balanced'], \n",
    "    'logisticregression__C': 10.0 ** np.arange(-2, 2, 0.5), #try with -2,2,0.5\n",
    "}\n",
    "\n",
    "pipe_lr_max_iter = make_pipeline(preprocessor, LogisticRegression(max_iter=10000))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49175283989178376\n",
      "{'logisticregression__C': 0.31622776601683794, 'logisticregression__class_weight': {0: 1, 1: 3}}\n"
     ]
    }
   ],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    pipe_lr_max_iter, param_grid, cv=5, n_jobs=-1, return_train_score = True, scoring=\"f1\"\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.391509</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.494279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.635112</td>\n",
       "      <td>0.007869</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.485682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.755978</td>\n",
       "      <td>0.009566</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.531108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.353360</td>\n",
       "      <td>0.009740</td>\n",
       "      <td>0.478528</td>\n",
       "      <td>0.538226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.117030</td>\n",
       "      <td>0.008415</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.535604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  4.391509    0.008161    0.468293     0.494279\n",
       "1  4.635112    0.007869    0.504587     0.485682\n",
       "2  6.755978    0.009566    0.428571     0.531108\n",
       "3  7.353360    0.009740    0.478528     0.538226\n",
       "4  6.117030    0.008415    0.453488     0.535604"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_grid = cross_validate(\n",
    "    grid_search, X_train, y_train, return_train_score=True, scoring=\"f1\"\n",
    ")\n",
    "\n",
    "\n",
    "pd.DataFrame(scores_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>rank_test_score</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean_fit_time</th>\n",
       "      <td>0.097179</td>\n",
       "      <td>0.070871</td>\n",
       "      <td>0.118647</td>\n",
       "      <td>0.066362</td>\n",
       "      <td>0.198623</td>\n",
       "      <td>0.053042</td>\n",
       "      <td>0.194653</td>\n",
       "      <td>0.171127</td>\n",
       "      <td>0.089065</td>\n",
       "      <td>0.050443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140877</td>\n",
       "      <td>0.052962</td>\n",
       "      <td>0.212282</td>\n",
       "      <td>0.142777</td>\n",
       "      <td>0.256674</td>\n",
       "      <td>0.103056</td>\n",
       "      <td>0.07459</td>\n",
       "      <td>0.061612</td>\n",
       "      <td>0.045902</td>\n",
       "      <td>0.04962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_fit_time</th>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.008338</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.049162</td>\n",
       "      <td>0.00492</td>\n",
       "      <td>0.043774</td>\n",
       "      <td>0.016862</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.007472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020544</td>\n",
       "      <td>0.00296</td>\n",
       "      <td>0.027958</td>\n",
       "      <td>0.018155</td>\n",
       "      <td>0.063815</td>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.005428</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_score_time</th>\n",
       "      <td>0.015966</td>\n",
       "      <td>0.016685</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.016425</td>\n",
       "      <td>0.018149</td>\n",
       "      <td>0.01545</td>\n",
       "      <td>0.016598</td>\n",
       "      <td>0.021347</td>\n",
       "      <td>0.014437</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019344</td>\n",
       "      <td>0.016737</td>\n",
       "      <td>0.016405</td>\n",
       "      <td>0.015094</td>\n",
       "      <td>0.018606</td>\n",
       "      <td>0.014536</td>\n",
       "      <td>0.015829</td>\n",
       "      <td>0.015712</td>\n",
       "      <td>0.013931</td>\n",
       "      <td>0.015911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_score_time</th>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.00369</td>\n",
       "      <td>0.002192</td>\n",
       "      <td>0.002537</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.00374</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005066</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.002125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_logisticregression__C</th>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>31.622777</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.01</td>\n",
       "      <td>...</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.162278</td>\n",
       "      <td>31.622777</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.316228</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.031623</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_logisticregression__class_weight</th>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>balanced</td>\n",
       "      <td>balanced</td>\n",
       "      <td>...</td>\n",
       "      <td>balanced</td>\n",
       "      <td>{0: 1, 1: 3}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <td>{'logisticregression__C': 0.31622776601683794,...</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>{'logisticregression__C': 0.03162277660168379,...</td>\n",
       "      <td>{'logisticregression__C': 31.622776601683793, ...</td>\n",
       "      <td>{'logisticregression__C': 3.1622776601683795, ...</td>\n",
       "      <td>{'logisticregression__C': 0.31622776601683794,...</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>...</td>\n",
       "      <td>{'logisticregression__C': 3.1622776601683795, ...</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "      <td>{'logisticregression__C': 10.0, 'logisticregre...</td>\n",
       "      <td>{'logisticregression__C': 3.1622776601683795, ...</td>\n",
       "      <td>{'logisticregression__C': 31.622776601683793, ...</td>\n",
       "      <td>{'logisticregression__C': 1.0, 'logisticregres...</td>\n",
       "      <td>{'logisticregression__C': 0.31622776601683794,...</td>\n",
       "      <td>{'logisticregression__C': 0.1, 'logisticregres...</td>\n",
       "      <td>{'logisticregression__C': 0.03162277660168379,...</td>\n",
       "      <td>{'logisticregression__C': 0.01, 'logisticregre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_test_score</th>\n",
       "      <td>0.524823</td>\n",
       "      <td>0.546763</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.521127</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.465116</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.357895</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.054795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_test_score</th>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.552147</td>\n",
       "      <td>0.54321</td>\n",
       "      <td>0.495327</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.559006</td>\n",
       "      <td>0.540881</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.482143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488038</td>\n",
       "      <td>0.475524</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.202247</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.027397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_test_score</th>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.422535</td>\n",
       "      <td>0.427673</td>\n",
       "      <td>0.478049</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.468293</td>\n",
       "      <td>0.419753</td>\n",
       "      <td>0.42236</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.403101</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.252632</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.255319</td>\n",
       "      <td>0.234043</td>\n",
       "      <td>0.26087</td>\n",
       "      <td>0.211765</td>\n",
       "      <td>0.106667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_test_score</th>\n",
       "      <td>0.478528</td>\n",
       "      <td>0.466258</td>\n",
       "      <td>0.45509</td>\n",
       "      <td>0.477477</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.469027</td>\n",
       "      <td>0.458824</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.450216</td>\n",
       "      <td>...</td>\n",
       "      <td>0.453704</td>\n",
       "      <td>0.457516</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.368932</td>\n",
       "      <td>0.371134</td>\n",
       "      <td>0.336842</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.109589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_test_score</th>\n",
       "      <td>0.467836</td>\n",
       "      <td>0.445783</td>\n",
       "      <td>0.453488</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>0.434286</td>\n",
       "      <td>0.480349</td>\n",
       "      <td>0.429379</td>\n",
       "      <td>0.44186</td>\n",
       "      <td>0.436681</td>\n",
       "      <td>0.471616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.442396</td>\n",
       "      <td>0.413333</td>\n",
       "      <td>0.283019</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.280374</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.236559</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.028169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_test_score</th>\n",
       "      <td>0.491753</td>\n",
       "      <td>0.486697</td>\n",
       "      <td>0.481606</td>\n",
       "      <td>0.481274</td>\n",
       "      <td>0.478599</td>\n",
       "      <td>0.47811</td>\n",
       "      <td>0.477618</td>\n",
       "      <td>0.477553</td>\n",
       "      <td>0.470961</td>\n",
       "      <td>0.470091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460981</td>\n",
       "      <td>0.442918</td>\n",
       "      <td>0.314864</td>\n",
       "      <td>0.310324</td>\n",
       "      <td>0.309276</td>\n",
       "      <td>0.305093</td>\n",
       "      <td>0.287234</td>\n",
       "      <td>0.274248</td>\n",
       "      <td>0.201308</td>\n",
       "      <td>0.065323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_test_score</th>\n",
       "      <td>0.045509</td>\n",
       "      <td>0.053104</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.010457</td>\n",
       "      <td>0.053088</td>\n",
       "      <td>0.014009</td>\n",
       "      <td>0.053936</td>\n",
       "      <td>0.045795</td>\n",
       "      <td>0.020511</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019334</td>\n",
       "      <td>0.029085</td>\n",
       "      <td>0.045449</td>\n",
       "      <td>0.045485</td>\n",
       "      <td>0.050331</td>\n",
       "      <td>0.055012</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>0.05871</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>0.036327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split0_train_score</th>\n",
       "      <td>0.504644</td>\n",
       "      <td>0.494453</td>\n",
       "      <td>0.504587</td>\n",
       "      <td>0.498845</td>\n",
       "      <td>0.506829</td>\n",
       "      <td>0.494279</td>\n",
       "      <td>0.509091</td>\n",
       "      <td>0.507599</td>\n",
       "      <td>0.498254</td>\n",
       "      <td>0.479911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498262</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.351421</td>\n",
       "      <td>0.34715</td>\n",
       "      <td>0.357513</td>\n",
       "      <td>0.335958</td>\n",
       "      <td>0.318059</td>\n",
       "      <td>0.265537</td>\n",
       "      <td>0.167702</td>\n",
       "      <td>0.061856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split1_train_score</th>\n",
       "      <td>0.523659</td>\n",
       "      <td>0.512821</td>\n",
       "      <td>0.520376</td>\n",
       "      <td>0.497674</td>\n",
       "      <td>0.528594</td>\n",
       "      <td>0.485682</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.498829</td>\n",
       "      <td>0.469136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506944</td>\n",
       "      <td>0.467257</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.377551</td>\n",
       "      <td>0.380711</td>\n",
       "      <td>0.37018</td>\n",
       "      <td>0.360313</td>\n",
       "      <td>0.321716</td>\n",
       "      <td>0.240469</td>\n",
       "      <td>0.081633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split2_train_score</th>\n",
       "      <td>0.531108</td>\n",
       "      <td>0.51944</td>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.511521</td>\n",
       "      <td>0.537538</td>\n",
       "      <td>0.494279</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>0.538346</td>\n",
       "      <td>0.517401</td>\n",
       "      <td>0.481356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513419</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.382134</td>\n",
       "      <td>0.386308</td>\n",
       "      <td>0.372449</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.313187</td>\n",
       "      <td>0.23494</td>\n",
       "      <td>0.075342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split3_train_score</th>\n",
       "      <td>0.538226</td>\n",
       "      <td>0.524031</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.515081</td>\n",
       "      <td>0.536364</td>\n",
       "      <td>0.493656</td>\n",
       "      <td>0.537764</td>\n",
       "      <td>0.534954</td>\n",
       "      <td>0.519389</td>\n",
       "      <td>0.484091</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523754</td>\n",
       "      <td>0.480274</td>\n",
       "      <td>0.348485</td>\n",
       "      <td>0.349367</td>\n",
       "      <td>0.352645</td>\n",
       "      <td>0.362245</td>\n",
       "      <td>0.349869</td>\n",
       "      <td>0.319783</td>\n",
       "      <td>0.193353</td>\n",
       "      <td>0.07483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split4_train_score</th>\n",
       "      <td>0.528951</td>\n",
       "      <td>0.513557</td>\n",
       "      <td>0.535604</td>\n",
       "      <td>0.516432</td>\n",
       "      <td>0.557078</td>\n",
       "      <td>0.499422</td>\n",
       "      <td>0.559271</td>\n",
       "      <td>0.550769</td>\n",
       "      <td>0.524203</td>\n",
       "      <td>0.484501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530035</td>\n",
       "      <td>0.482639</td>\n",
       "      <td>0.409877</td>\n",
       "      <td>0.406015</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.378517</td>\n",
       "      <td>0.341207</td>\n",
       "      <td>0.312668</td>\n",
       "      <td>0.203593</td>\n",
       "      <td>0.087248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_train_score</th>\n",
       "      <td>0.525318</td>\n",
       "      <td>0.51286</td>\n",
       "      <td>0.525813</td>\n",
       "      <td>0.507911</td>\n",
       "      <td>0.53328</td>\n",
       "      <td>0.493464</td>\n",
       "      <td>0.535216</td>\n",
       "      <td>0.531597</td>\n",
       "      <td>0.511615</td>\n",
       "      <td>0.479799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514483</td>\n",
       "      <td>0.473504</td>\n",
       "      <td>0.373329</td>\n",
       "      <td>0.372443</td>\n",
       "      <td>0.378769</td>\n",
       "      <td>0.36387</td>\n",
       "      <td>0.342855</td>\n",
       "      <td>0.306578</td>\n",
       "      <td>0.208011</td>\n",
       "      <td>0.076182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_train_score</th>\n",
       "      <td>0.011345</td>\n",
       "      <td>0.010074</td>\n",
       "      <td>0.012077</td>\n",
       "      <td>0.00805</td>\n",
       "      <td>0.016221</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.016308</td>\n",
       "      <td>0.01434</td>\n",
       "      <td>0.010903</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011392</td>\n",
       "      <td>0.014881</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.021997</td>\n",
       "      <td>0.02294</td>\n",
       "      <td>0.014897</td>\n",
       "      <td>0.013967</td>\n",
       "      <td>0.020826</td>\n",
       "      <td>0.026974</td>\n",
       "      <td>0.008485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "rank_test_score                                                                        1   \\\n",
       "mean_fit_time                                                                    0.097179   \n",
       "std_fit_time                                                                     0.007954   \n",
       "mean_score_time                                                                  0.015966   \n",
       "std_score_time                                                                   0.001938   \n",
       "param_logisticregression__C                                                      0.316228   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 0.31622776601683794,...   \n",
       "split0_test_score                                                                0.524823   \n",
       "split1_test_score                                                                0.559006   \n",
       "split2_test_score                                                                0.428571   \n",
       "split3_test_score                                                                0.478528   \n",
       "split4_test_score                                                                0.467836   \n",
       "mean_test_score                                                                  0.491753   \n",
       "std_test_score                                                                   0.045509   \n",
       "split0_train_score                                                               0.504644   \n",
       "split1_train_score                                                               0.523659   \n",
       "split2_train_score                                                               0.531108   \n",
       "split3_train_score                                                               0.538226   \n",
       "split4_train_score                                                               0.528951   \n",
       "mean_train_score                                                                 0.525318   \n",
       "std_train_score                                                                  0.011345   \n",
       "\n",
       "rank_test_score                                                                        2   \\\n",
       "mean_fit_time                                                                    0.070871   \n",
       "std_fit_time                                                                     0.008338   \n",
       "mean_score_time                                                                  0.016685   \n",
       "std_score_time                                                                    0.00369   \n",
       "param_logisticregression__C                                                           0.1   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "split0_test_score                                                                0.546763   \n",
       "split1_test_score                                                                0.552147   \n",
       "split2_test_score                                                                0.422535   \n",
       "split3_test_score                                                                0.466258   \n",
       "split4_test_score                                                                0.445783   \n",
       "mean_test_score                                                                  0.486697   \n",
       "std_test_score                                                                   0.053104   \n",
       "split0_train_score                                                               0.494453   \n",
       "split1_train_score                                                               0.512821   \n",
       "split2_train_score                                                                0.51944   \n",
       "split3_train_score                                                               0.524031   \n",
       "split4_train_score                                                               0.513557   \n",
       "mean_train_score                                                                  0.51286   \n",
       "std_train_score                                                                  0.010074   \n",
       "\n",
       "rank_test_score                                                                        3   \\\n",
       "mean_fit_time                                                                    0.118647   \n",
       "std_fit_time                                                                     0.007888   \n",
       "mean_score_time                                                                  0.015917   \n",
       "std_score_time                                                                   0.002192   \n",
       "param_logisticregression__C                                                           1.0   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "split0_test_score                                                                0.528571   \n",
       "split1_test_score                                                                 0.54321   \n",
       "split2_test_score                                                                0.427673   \n",
       "split3_test_score                                                                 0.45509   \n",
       "split4_test_score                                                                0.453488   \n",
       "mean_test_score                                                                  0.481606   \n",
       "std_test_score                                                                   0.045614   \n",
       "split0_train_score                                                               0.504587   \n",
       "split1_train_score                                                               0.520376   \n",
       "split2_train_score                                                               0.531915   \n",
       "split3_train_score                                                               0.536585   \n",
       "split4_train_score                                                               0.535604   \n",
       "mean_train_score                                                                 0.525813   \n",
       "std_train_score                                                                  0.012077   \n",
       "\n",
       "rank_test_score                                                                        4   \\\n",
       "mean_fit_time                                                                    0.066362   \n",
       "std_fit_time                                                                     0.006673   \n",
       "mean_score_time                                                                  0.016425   \n",
       "std_score_time                                                                   0.002537   \n",
       "param_logisticregression__C                                                           0.1   \n",
       "param_logisticregression__class_weight                                           balanced   \n",
       "params                                  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "split0_test_score                                                                    0.49   \n",
       "split1_test_score                                                                0.495327   \n",
       "split2_test_score                                                                0.478049   \n",
       "split3_test_score                                                                0.477477   \n",
       "split4_test_score                                                                0.465517   \n",
       "mean_test_score                                                                  0.481274   \n",
       "std_test_score                                                                   0.010457   \n",
       "split0_train_score                                                               0.498845   \n",
       "split1_train_score                                                               0.497674   \n",
       "split2_train_score                                                               0.511521   \n",
       "split3_train_score                                                               0.515081   \n",
       "split4_train_score                                                               0.516432   \n",
       "mean_train_score                                                                 0.507911   \n",
       "std_train_score                                                                   0.00805   \n",
       "\n",
       "rank_test_score                                                                        5   \\\n",
       "mean_fit_time                                                                    0.198623   \n",
       "std_fit_time                                                                     0.049162   \n",
       "mean_score_time                                                                  0.018149   \n",
       "std_score_time                                                                   0.003622   \n",
       "param_logisticregression__C                                                          10.0   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "split0_test_score                                                                0.521127   \n",
       "split1_test_score                                                                0.559006   \n",
       "split2_test_score                                                                0.419753   \n",
       "split3_test_score                                                                0.458824   \n",
       "split4_test_score                                                                0.434286   \n",
       "mean_test_score                                                                  0.478599   \n",
       "std_test_score                                                                   0.053088   \n",
       "split0_train_score                                                               0.506829   \n",
       "split1_train_score                                                               0.528594   \n",
       "split2_train_score                                                               0.537538   \n",
       "split3_train_score                                                               0.536364   \n",
       "split4_train_score                                                               0.557078   \n",
       "mean_train_score                                                                  0.53328   \n",
       "std_train_score                                                                  0.016221   \n",
       "\n",
       "rank_test_score                                                                        6   \\\n",
       "mean_fit_time                                                                    0.053042   \n",
       "std_fit_time                                                                      0.00492   \n",
       "mean_score_time                                                                   0.01545   \n",
       "std_score_time                                                                   0.001122   \n",
       "param_logisticregression__C                                                      0.031623   \n",
       "param_logisticregression__class_weight                                           balanced   \n",
       "params                                  {'logisticregression__C': 0.03162277660168379,...   \n",
       "split0_test_score                                                                0.468293   \n",
       "split1_test_score                                                                0.504587   \n",
       "split2_test_score                                                                0.468293   \n",
       "split3_test_score                                                                0.469027   \n",
       "split4_test_score                                                                0.480349   \n",
       "mean_test_score                                                                   0.47811   \n",
       "std_test_score                                                                   0.014009   \n",
       "split0_train_score                                                               0.494279   \n",
       "split1_train_score                                                               0.485682   \n",
       "split2_train_score                                                               0.494279   \n",
       "split3_train_score                                                               0.493656   \n",
       "split4_train_score                                                               0.499422   \n",
       "mean_train_score                                                                 0.493464   \n",
       "std_train_score                                                                  0.004414   \n",
       "\n",
       "rank_test_score                                                                        7   \\\n",
       "mean_fit_time                                                                    0.194653   \n",
       "std_fit_time                                                                     0.043774   \n",
       "mean_score_time                                                                  0.016598   \n",
       "std_score_time                                                                   0.001253   \n",
       "param_logisticregression__C                                                     31.622777   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 31.622776601683793, ...   \n",
       "split0_test_score                                                                0.521127   \n",
       "split1_test_score                                                                0.559006   \n",
       "split2_test_score                                                                0.419753   \n",
       "split3_test_score                                                                0.458824   \n",
       "split4_test_score                                                                0.429379   \n",
       "mean_test_score                                                                  0.477618   \n",
       "std_test_score                                                                   0.053936   \n",
       "split0_train_score                                                               0.509091   \n",
       "split1_train_score                                                               0.529412   \n",
       "split2_train_score                                                               0.540541   \n",
       "split3_train_score                                                               0.537764   \n",
       "split4_train_score                                                               0.559271   \n",
       "mean_train_score                                                                 0.535216   \n",
       "std_train_score                                                                  0.016308   \n",
       "\n",
       "rank_test_score                                                                        8   \\\n",
       "mean_fit_time                                                                    0.171127   \n",
       "std_fit_time                                                                     0.016862   \n",
       "mean_score_time                                                                  0.021347   \n",
       "std_score_time                                                                    0.00374   \n",
       "param_logisticregression__C                                                      3.162278   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 3.1622776601683795, ...   \n",
       "split0_test_score                                                                0.521127   \n",
       "split1_test_score                                                                0.540881   \n",
       "split2_test_score                                                                 0.42236   \n",
       "split3_test_score                                                                0.461538   \n",
       "split4_test_score                                                                 0.44186   \n",
       "mean_test_score                                                                  0.477553   \n",
       "std_test_score                                                                   0.045795   \n",
       "split0_train_score                                                               0.507599   \n",
       "split1_train_score                                                               0.526316   \n",
       "split2_train_score                                                               0.538346   \n",
       "split3_train_score                                                               0.534954   \n",
       "split4_train_score                                                               0.550769   \n",
       "mean_train_score                                                                 0.531597   \n",
       "std_train_score                                                                   0.01434   \n",
       "\n",
       "rank_test_score                                                                        9   \\\n",
       "mean_fit_time                                                                    0.089065   \n",
       "std_fit_time                                                                       0.0055   \n",
       "mean_score_time                                                                  0.014437   \n",
       "std_score_time                                                                   0.002133   \n",
       "param_logisticregression__C                                                      0.316228   \n",
       "param_logisticregression__class_weight                                           balanced   \n",
       "params                                  {'logisticregression__C': 0.31622776601683794,...   \n",
       "split0_test_score                                                                0.484536   \n",
       "split1_test_score                                                                0.497653   \n",
       "split2_test_score                                                                0.465347   \n",
       "split3_test_score                                                                0.470588   \n",
       "split4_test_score                                                                0.436681   \n",
       "mean_test_score                                                                  0.470961   \n",
       "std_test_score                                                                   0.020511   \n",
       "split0_train_score                                                               0.498254   \n",
       "split1_train_score                                                               0.498829   \n",
       "split2_train_score                                                               0.517401   \n",
       "split3_train_score                                                               0.519389   \n",
       "split4_train_score                                                               0.524203   \n",
       "mean_train_score                                                                 0.511615   \n",
       "std_train_score                                                                  0.010903   \n",
       "\n",
       "rank_test_score                                                                        10  \\\n",
       "mean_fit_time                                                                    0.050443   \n",
       "std_fit_time                                                                     0.007472   \n",
       "mean_score_time                                                                  0.016678   \n",
       "std_score_time                                                                   0.004385   \n",
       "param_logisticregression__C                                                          0.01   \n",
       "param_logisticregression__class_weight                                           balanced   \n",
       "params                                  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "split0_test_score                                                                0.481132   \n",
       "split1_test_score                                                                0.482143   \n",
       "split2_test_score                                                                0.465347   \n",
       "split3_test_score                                                                0.450216   \n",
       "split4_test_score                                                                0.471616   \n",
       "mean_test_score                                                                  0.470091   \n",
       "std_test_score                                                                   0.011722   \n",
       "split0_train_score                                                               0.479911   \n",
       "split1_train_score                                                               0.469136   \n",
       "split2_train_score                                                               0.481356   \n",
       "split3_train_score                                                               0.484091   \n",
       "split4_train_score                                                               0.484501   \n",
       "mean_train_score                                                                 0.479799   \n",
       "std_train_score                                                                  0.005598   \n",
       "\n",
       "rank_test_score                         ...  \\\n",
       "mean_fit_time                           ...   \n",
       "std_fit_time                            ...   \n",
       "mean_score_time                         ...   \n",
       "std_score_time                          ...   \n",
       "param_logisticregression__C             ...   \n",
       "param_logisticregression__class_weight  ...   \n",
       "params                                  ...   \n",
       "split0_test_score                       ...   \n",
       "split1_test_score                       ...   \n",
       "split2_test_score                       ...   \n",
       "split3_test_score                       ...   \n",
       "split4_test_score                       ...   \n",
       "mean_test_score                         ...   \n",
       "std_test_score                          ...   \n",
       "split0_train_score                      ...   \n",
       "split1_train_score                      ...   \n",
       "split2_train_score                      ...   \n",
       "split3_train_score                      ...   \n",
       "split4_train_score                      ...   \n",
       "mean_train_score                        ...   \n",
       "std_train_score                         ...   \n",
       "\n",
       "rank_test_score                                                                        15  \\\n",
       "mean_fit_time                                                                    0.140877   \n",
       "std_fit_time                                                                     0.020544   \n",
       "mean_score_time                                                                  0.019344   \n",
       "std_score_time                                                                   0.005066   \n",
       "param_logisticregression__C                                                      3.162278   \n",
       "param_logisticregression__class_weight                                           balanced   \n",
       "params                                  {'logisticregression__C': 3.1622776601683795, ...   \n",
       "split0_test_score                                                                0.479592   \n",
       "split1_test_score                                                                0.488038   \n",
       "split2_test_score                                                                0.441176   \n",
       "split3_test_score                                                                0.453704   \n",
       "split4_test_score                                                                0.442396   \n",
       "mean_test_score                                                                  0.460981   \n",
       "std_test_score                                                                   0.019334   \n",
       "split0_train_score                                                               0.498262   \n",
       "split1_train_score                                                               0.506944   \n",
       "split2_train_score                                                               0.513419   \n",
       "split3_train_score                                                               0.523754   \n",
       "split4_train_score                                                               0.530035   \n",
       "mean_train_score                                                                 0.514483   \n",
       "std_train_score                                                                  0.011392   \n",
       "\n",
       "rank_test_score                                                                        16  \\\n",
       "mean_fit_time                                                                    0.052962   \n",
       "std_fit_time                                                                      0.00296   \n",
       "mean_score_time                                                                  0.016737   \n",
       "std_score_time                                                                   0.001923   \n",
       "param_logisticregression__C                                                          0.01   \n",
       "param_logisticregression__class_weight                                       {0: 1, 1: 3}   \n",
       "params                                  {'logisticregression__C': 0.01, 'logisticregre...   \n",
       "split0_test_score                                                                0.465116   \n",
       "split1_test_score                                                                0.475524   \n",
       "split2_test_score                                                                0.403101   \n",
       "split3_test_score                                                                0.457516   \n",
       "split4_test_score                                                                0.413333   \n",
       "mean_test_score                                                                  0.442918   \n",
       "std_test_score                                                                   0.029085   \n",
       "split0_train_score                                                               0.447552   \n",
       "split1_train_score                                                               0.467257   \n",
       "split2_train_score                                                               0.489796   \n",
       "split3_train_score                                                               0.480274   \n",
       "split4_train_score                                                               0.482639   \n",
       "mean_train_score                                                                 0.473504   \n",
       "std_train_score                                                                  0.014881   \n",
       "\n",
       "rank_test_score                                                                        17  \\\n",
       "mean_fit_time                                                                    0.212282   \n",
       "std_fit_time                                                                     0.027958   \n",
       "mean_score_time                                                                  0.016405   \n",
       "std_score_time                                                                   0.001582   \n",
       "param_logisticregression__C                                                          10.0   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 10.0, 'logisticregre...   \n",
       "split0_test_score                                                                    0.36   \n",
       "split1_test_score                                                                0.306122   \n",
       "split2_test_score                                                                0.252632   \n",
       "split3_test_score                                                                0.372549   \n",
       "split4_test_score                                                                0.283019   \n",
       "mean_test_score                                                                  0.314864   \n",
       "std_test_score                                                                   0.045449   \n",
       "split0_train_score                                                               0.351421   \n",
       "split1_train_score                                                               0.377551   \n",
       "split2_train_score                                                                0.37931   \n",
       "split3_train_score                                                               0.348485   \n",
       "split4_train_score                                                               0.409877   \n",
       "mean_train_score                                                                 0.373329   \n",
       "std_train_score                                                                    0.0223   \n",
       "\n",
       "rank_test_score                                                                        18  \\\n",
       "mean_fit_time                                                                    0.142777   \n",
       "std_fit_time                                                                     0.018155   \n",
       "mean_score_time                                                                  0.015094   \n",
       "std_score_time                                                                   0.002776   \n",
       "param_logisticregression__C                                                      3.162278   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 3.1622776601683795, ...   \n",
       "split0_test_score                                                                    0.36   \n",
       "split1_test_score                                                                0.306122   \n",
       "split2_test_score                                                                0.252632   \n",
       "split3_test_score                                                                0.363636   \n",
       "split4_test_score                                                                0.269231   \n",
       "mean_test_score                                                                  0.310324   \n",
       "std_test_score                                                                   0.045485   \n",
       "split0_train_score                                                                0.34715   \n",
       "split1_train_score                                                               0.377551   \n",
       "split2_train_score                                                               0.382134   \n",
       "split3_train_score                                                               0.349367   \n",
       "split4_train_score                                                               0.406015   \n",
       "mean_train_score                                                                 0.372443   \n",
       "std_train_score                                                                  0.021997   \n",
       "\n",
       "rank_test_score                                                                        19  \\\n",
       "mean_fit_time                                                                    0.256674   \n",
       "std_fit_time                                                                     0.063815   \n",
       "mean_score_time                                                                  0.018606   \n",
       "std_score_time                                                                   0.003479   \n",
       "param_logisticregression__C                                                     31.622777   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 31.622776601683793, ...   \n",
       "split0_test_score                                                                    0.36   \n",
       "split1_test_score                                                                 0.30303   \n",
       "split2_test_score                                                                0.234043   \n",
       "split3_test_score                                                                0.368932   \n",
       "split4_test_score                                                                0.280374   \n",
       "mean_test_score                                                                  0.309276   \n",
       "std_test_score                                                                   0.050331   \n",
       "split0_train_score                                                               0.357513   \n",
       "split1_train_score                                                               0.380711   \n",
       "split2_train_score                                                               0.386308   \n",
       "split3_train_score                                                               0.352645   \n",
       "split4_train_score                                                               0.416667   \n",
       "mean_train_score                                                                 0.378769   \n",
       "std_train_score                                                                   0.02294   \n",
       "\n",
       "rank_test_score                                                                        20  \\\n",
       "mean_fit_time                                                                    0.103056   \n",
       "std_fit_time                                                                     0.010493   \n",
       "mean_score_time                                                                  0.014536   \n",
       "std_score_time                                                                   0.001767   \n",
       "param_logisticregression__C                                                           1.0   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 1.0, 'logisticregres...   \n",
       "split0_test_score                                                                0.367347   \n",
       "split1_test_score                                                                0.291667   \n",
       "split2_test_score                                                                0.255319   \n",
       "split3_test_score                                                                0.371134   \n",
       "split4_test_score                                                                    0.24   \n",
       "mean_test_score                                                                  0.305093   \n",
       "std_test_score                                                                   0.055012   \n",
       "split0_train_score                                                               0.335958   \n",
       "split1_train_score                                                                0.37018   \n",
       "split2_train_score                                                               0.372449   \n",
       "split3_train_score                                                               0.362245   \n",
       "split4_train_score                                                               0.378517   \n",
       "mean_train_score                                                                  0.36387   \n",
       "std_train_score                                                                  0.014897   \n",
       "\n",
       "rank_test_score                                                                        21  \\\n",
       "mean_fit_time                                                                     0.07459   \n",
       "std_fit_time                                                                     0.005428   \n",
       "mean_score_time                                                                  0.015829   \n",
       "std_score_time                                                                   0.003806   \n",
       "param_logisticregression__C                                                      0.316228   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 0.31622776601683794,...   \n",
       "split0_test_score                                                                0.357895   \n",
       "split1_test_score                                                                0.270833   \n",
       "split2_test_score                                                                0.234043   \n",
       "split3_test_score                                                                0.336842   \n",
       "split4_test_score                                                                0.236559   \n",
       "mean_test_score                                                                  0.287234   \n",
       "std_test_score                                                                   0.051225   \n",
       "split0_train_score                                                               0.318059   \n",
       "split1_train_score                                                               0.360313   \n",
       "split2_train_score                                                               0.344828   \n",
       "split3_train_score                                                               0.349869   \n",
       "split4_train_score                                                               0.341207   \n",
       "mean_train_score                                                                 0.342855   \n",
       "std_train_score                                                                  0.013967   \n",
       "\n",
       "rank_test_score                                                                        22  \\\n",
       "mean_fit_time                                                                    0.061612   \n",
       "std_fit_time                                                                     0.004511   \n",
       "mean_score_time                                                                  0.015712   \n",
       "std_score_time                                                                   0.002268   \n",
       "param_logisticregression__C                                                           0.1   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 0.1, 'logisticregres...   \n",
       "split0_test_score                                                                0.361702   \n",
       "split1_test_score                                                                0.202247   \n",
       "split2_test_score                                                                 0.26087   \n",
       "split3_test_score                                                                0.319149   \n",
       "split4_test_score                                                                0.227273   \n",
       "mean_test_score                                                                  0.274248   \n",
       "std_test_score                                                                    0.05871   \n",
       "split0_train_score                                                               0.265537   \n",
       "split1_train_score                                                               0.321716   \n",
       "split2_train_score                                                               0.313187   \n",
       "split3_train_score                                                               0.319783   \n",
       "split4_train_score                                                               0.312668   \n",
       "mean_train_score                                                                 0.306578   \n",
       "std_train_score                                                                  0.020826   \n",
       "\n",
       "rank_test_score                                                                        23  \\\n",
       "mean_fit_time                                                                    0.045902   \n",
       "std_fit_time                                                                     0.004301   \n",
       "mean_score_time                                                                  0.013931   \n",
       "std_score_time                                                                   0.002139   \n",
       "param_logisticregression__C                                                      0.031623   \n",
       "param_logisticregression__class_weight                                               None   \n",
       "params                                  {'logisticregression__C': 0.03162277660168379,...   \n",
       "split0_test_score                                                                0.219512   \n",
       "split1_test_score                                                                0.141176   \n",
       "split2_test_score                                                                0.211765   \n",
       "split3_test_score                                                                0.305882   \n",
       "split4_test_score                                                                0.128205   \n",
       "mean_test_score                                                                  0.201308   \n",
       "std_test_score                                                                   0.063775   \n",
       "split0_train_score                                                               0.167702   \n",
       "split1_train_score                                                               0.240469   \n",
       "split2_train_score                                                                0.23494   \n",
       "split3_train_score                                                               0.193353   \n",
       "split4_train_score                                                               0.203593   \n",
       "mean_train_score                                                                 0.208011   \n",
       "std_train_score                                                                  0.026974   \n",
       "\n",
       "rank_test_score                                                                        24  \n",
       "mean_fit_time                                                                     0.04962  \n",
       "std_fit_time                                                                     0.003528  \n",
       "mean_score_time                                                                  0.015911  \n",
       "std_score_time                                                                   0.002125  \n",
       "param_logisticregression__C                                                          0.01  \n",
       "param_logisticregression__class_weight                                               None  \n",
       "params                                  {'logisticregression__C': 0.01, 'logisticregre...  \n",
       "split0_test_score                                                                0.054795  \n",
       "split1_test_score                                                                0.027397  \n",
       "split2_test_score                                                                0.106667  \n",
       "split3_test_score                                                                0.109589  \n",
       "split4_test_score                                                                0.028169  \n",
       "mean_test_score                                                                  0.065323  \n",
       "std_test_score                                                                   0.036327  \n",
       "split0_train_score                                                               0.061856  \n",
       "split1_train_score                                                               0.081633  \n",
       "split2_train_score                                                               0.075342  \n",
       "split3_train_score                                                                0.07483  \n",
       "split4_train_score                                                               0.087248  \n",
       "mean_train_score                                                                 0.076182  \n",
       "std_train_score                                                                  0.008485  \n",
       "\n",
       "[21 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid_search.cv_results_).set_index(\"rank_test_score\").sort_index()\n",
    "results.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The two optimal hyper parameters found were C = 0.31622776601683794 and class_weight = {0: 1, 1: 3}. The best validation f1 score was 0.491753 on the test split.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Test results\n",
    "rubric={points:10}\n",
    "\n",
    "**Your tasks**\n",
    "1. Evaluate the best model on the test set. In particular show each of the following on the test set:  \n",
    "    - Plot Confusion matrix\n",
    "    - Plot Precision-recall curve \n",
    "    - Calculate average precision score\n",
    "    - Plot ROC curve\n",
    "    - Report AUC score\n",
    "3. Comment on the AUC score and give an intuitive explanation of what this value of AUC means for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code adapted from lecture 9\n",
    "pipe = make_pipeline(preprocessor, LogisticRegression(C = 0.31622776601683794, class_weight = {0: 1, 1: 3}, max_iter = 1000))\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pred = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x104537f40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(\n",
    "    pipe,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    display_labels=[\"No churn\", \"churn\"],\n",
    "    values_format = \"d\",\n",
    "    cmap = plt.cm.Blues,\n",
    "    colorbar = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(\n",
    "    y_test, grid_search.predict_proba(X_test)[:, 1]\n",
    ")\n",
    "plt.plot(precision, recall, label=\"logistic regression: PR curve\")\n",
    "plt.xlabel(\"Precision\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.plot(\n",
    "    precision_score(y_test, grid_search.predict(X_test)),\n",
    "    recall_score(y_test, grid_search.predict(X_test)),\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision of logistic regression: 0.467\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "ap_lr = average_precision_score(y_test, grid_search.predict_proba(X_test)[:, 1])\n",
    "print(\"Average precision of logistic regression: {:.3f}\".format(ap_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, grid_search.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"ROC Curve\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR (recall)\")\n",
    "\n",
    "default_threshold = np.argmin(np.abs(thresholds - 0.5))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[default_threshold],\n",
    "    tpr[default_threshold],\n",
    "    \"or\",\n",
    "    markersize=10,\n",
    "    label=\"threshold 0.5\",\n",
    ")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for logistic regression: 0.823\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "roc_lr = roc_auc_score(y_test, grid_search.predict_proba(X_test)[:, 1])\n",
    "print(\"AUC for logistic regression: {:.3f}\".format(roc_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An AUC score of 0.826 means that the model ranks 82.6% of positive examples higher than 82.6% of negative examples across the thresholds on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Regression metrics <a name=\"3\"></a>\n",
    "<hr> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we'll use [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) from `sklearn datasets`. The code below loads the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing_df = fetch_california_housing(as_frame=True).frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Data spitting and exploration \n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the data into train (75%) and test (25%) splits. \n",
    "2. Explore the train split. Do you need to apply any transformations on the data? If yes, create a preprocessor with the appropriate transformations. \n",
    "3. Separate `X` and `y` to train and test splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns = ['MedHouseVal'])\n",
    "y_train = train_df[\"MedHouseVal\"]\n",
    "\n",
    "X_test = test_df.drop(columns = ['MedHouseVal'])\n",
    "y_test = test_df[\"MedHouseVal\"]                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 15480 entries, 19995 to 19966\n",
      "Data columns (total 9 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   MedInc       15480 non-null  float64\n",
      " 1   HouseAge     15480 non-null  float64\n",
      " 2   AveRooms     15480 non-null  float64\n",
      " 3   AveBedrms    15480 non-null  float64\n",
      " 4   Population   15480 non-null  float64\n",
      " 5   AveOccup     15480 non-null  float64\n",
      " 6   Latitude     15480 non-null  float64\n",
      " 7   Longitude    15480 non-null  float64\n",
      " 8   MedHouseVal  15480 non-null  float64\n",
      "dtypes: float64(9)\n",
      "memory usage: 1.2 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>1.0349</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.165217</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>734.0</td>\n",
       "      <td>3.191304</td>\n",
       "      <td>36.19</td>\n",
       "      <td>-119.35</td>\n",
       "      <td>0.678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17889</th>\n",
       "      <td>4.7625</td>\n",
       "      <td>13.0</td>\n",
       "      <td>5.265207</td>\n",
       "      <td>1.002433</td>\n",
       "      <td>1087.0</td>\n",
       "      <td>2.644769</td>\n",
       "      <td>37.41</td>\n",
       "      <td>-121.95</td>\n",
       "      <td>1.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>3.5192</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.747475</td>\n",
       "      <td>1.845118</td>\n",
       "      <td>796.0</td>\n",
       "      <td>2.680135</td>\n",
       "      <td>38.61</td>\n",
       "      <td>-120.44</td>\n",
       "      <td>0.980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>2.8672</td>\n",
       "      <td>30.0</td>\n",
       "      <td>4.635616</td>\n",
       "      <td>1.090411</td>\n",
       "      <td>1130.0</td>\n",
       "      <td>3.095890</td>\n",
       "      <td>34.06</td>\n",
       "      <td>-118.13</td>\n",
       "      <td>1.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11247</th>\n",
       "      <td>4.1276</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.429936</td>\n",
       "      <td>0.963376</td>\n",
       "      <td>1749.0</td>\n",
       "      <td>2.785032</td>\n",
       "      <td>33.81</td>\n",
       "      <td>-118.00</td>\n",
       "      <td>1.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>3.6389</td>\n",
       "      <td>36.0</td>\n",
       "      <td>5.584615</td>\n",
       "      <td>1.115385</td>\n",
       "      <td>490.0</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>33.91</td>\n",
       "      <td>-118.10</td>\n",
       "      <td>1.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15377</th>\n",
       "      <td>4.5391</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.016688</td>\n",
       "      <td>1.017972</td>\n",
       "      <td>2436.0</td>\n",
       "      <td>3.127086</td>\n",
       "      <td>33.37</td>\n",
       "      <td>-117.24</td>\n",
       "      <td>1.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>5.6306</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.958393</td>\n",
       "      <td>1.031564</td>\n",
       "      <td>2435.0</td>\n",
       "      <td>3.493544</td>\n",
       "      <td>37.33</td>\n",
       "      <td>-121.76</td>\n",
       "      <td>2.862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>3.8750</td>\n",
       "      <td>44.0</td>\n",
       "      <td>4.739264</td>\n",
       "      <td>1.024540</td>\n",
       "      <td>561.0</td>\n",
       "      <td>1.720859</td>\n",
       "      <td>37.78</td>\n",
       "      <td>-122.44</td>\n",
       "      <td>4.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19966</th>\n",
       "      <td>2.5156</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.491379</td>\n",
       "      <td>1.117816</td>\n",
       "      <td>1241.0</td>\n",
       "      <td>3.566092</td>\n",
       "      <td>36.21</td>\n",
       "      <td>-119.08</td>\n",
       "      <td>0.593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15480 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "19995  1.0349       6.0  4.165217   0.982609       734.0  3.191304     36.19   \n",
       "17889  4.7625      13.0  5.265207   1.002433      1087.0  2.644769     37.41   \n",
       "1977   3.5192       9.0  8.747475   1.845118       796.0  2.680135     38.61   \n",
       "6861   2.8672      30.0  4.635616   1.090411      1130.0  3.095890     34.06   \n",
       "11247  4.1276      13.0  4.429936   0.963376      1749.0  2.785032     33.81   \n",
       "...       ...       ...       ...        ...         ...       ...       ...   \n",
       "7763   3.6389      36.0  5.584615   1.115385       490.0  3.769231     33.91   \n",
       "15377  4.5391      14.0  6.016688   1.017972      2436.0  3.127086     33.37   \n",
       "17730  5.6306       5.0  5.958393   1.031564      2435.0  3.493544     37.33   \n",
       "15725  3.8750      44.0  4.739264   1.024540       561.0  1.720859     37.78   \n",
       "19966  2.5156      20.0  5.491379   1.117816      1241.0  3.566092     36.21   \n",
       "\n",
       "       Longitude  MedHouseVal  \n",
       "19995    -119.35        0.678  \n",
       "17889    -121.95        1.375  \n",
       "1977     -120.44        0.980  \n",
       "6861     -118.13        1.985  \n",
       "11247    -118.00        1.538  \n",
       "...          ...          ...  \n",
       "7763     -118.10        1.676  \n",
       "15377    -117.24        1.809  \n",
       "17730    -121.76        2.862  \n",
       "15725    -122.44        4.125  \n",
       "19966    -119.08        0.593  \n",
       "\n",
       "[15480 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info()\n",
    "train_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_feats = [\"MedInc\", \"HouseAge\", \"AveRooms\", \"AveBedrms\", \"Population\", \"AveOccup\", \"Latitude\", \"Longitude\"]  \n",
    "numeric_transformer = make_pipeline(StandardScaler())\n",
    "preprocessor = make_column_transformer(\n",
    "    (numeric_transformer, numeric_feats)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all categories are numeric, and none have any missing values. The only transformations that need to occur are to use standard scaling.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Baseline: Linear Regression \n",
    "rubric={points:2}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Carry out cross-validation using `sklearn.linear_model.LinearRegression` with default scoring. \n",
    "2. What metric is used for scoring by default? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "      <th>train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.019126</td>\n",
       "      <td>0.017990</td>\n",
       "      <td>0.608633</td>\n",
       "      <td>0.603297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.011755</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.599232</td>\n",
       "      <td>0.607080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.011407</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.616812</td>\n",
       "      <td>0.602606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010778</td>\n",
       "      <td>0.004654</td>\n",
       "      <td>0.592940</td>\n",
       "      <td>0.607928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010483</td>\n",
       "      <td>0.004706</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.612855</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score  train_score\n",
       "0  0.019126    0.017990    0.608633     0.603297\n",
       "1  0.011755    0.004584    0.599232     0.607080\n",
       "2  0.011407    0.004834    0.616812     0.602606\n",
       "3  0.010778    0.004654    0.592940     0.607928\n",
       "4  0.010483    0.004706    0.014994     0.612855"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lreg = make_pipeline(preprocessor, Ridge())\n",
    "\n",
    "scores = cross_validate(\n",
    "    pipe_lreg, X_train, y_train, return_train_score = True\n",
    ")\n",
    "\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. R^2 score is used by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Random Forest Regressor\n",
    "rubric={points:7}\n",
    "\n",
    "In this exercise, we are going to use [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) model which we haven't looked into yet. At this point you should feel comfortable using models with our usual ML workflow even if you don't know the details. We'll talk about `RandomForestRegressor` later in the course.  \n",
    "\n",
    "The code below defines a custom scorer called `mape_scorer` and creates dictionaries for two model (`models`) and five evaluation metrics (`score_types_reg`). \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Using the `models` and the evaluation metrics `score_types_reg` in the code below, carry out cross-validation with each model, by passing the evaluation metrics to `scoring` argument of `cross_validate`. Use a pipeline with the model as an estimator if you are applying any transformations. \n",
    "2. Show results as a dataframe. \n",
    "3. Interpret the results. How do the models compare to the baseline? Which model seems to be performing well with different metrics? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "}\n",
    "\n",
    "score_types_reg = {\n",
    "    \"neg_mean_squared_error\": \"neg_mean_squared_error\",\n",
    "    \"neg_root_mean_squared_error\": \"neg_root_mean_squared_error\",\n",
    "    \"neg_mean_absolute_error\": \"neg_mean_absolute_error\",\n",
    "    \"r2\": \"r2\",\n",
    "    \"neg_mean_absolute_percentage_error\": \"neg_mean_absolute_percentage_error\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Ridge model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_squared_error</th>\n",
       "      <th>train_neg_mean_squared_error</th>\n",
       "      <th>test_neg_root_mean_squared_error</th>\n",
       "      <th>train_neg_root_mean_squared_error</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030982</td>\n",
       "      <td>0.005196</td>\n",
       "      <td>-0.549963</td>\n",
       "      <td>-0.523218</td>\n",
       "      <td>-0.741595</td>\n",
       "      <td>-0.723338</td>\n",
       "      <td>-0.539838</td>\n",
       "      <td>-0.529527</td>\n",
       "      <td>0.608633</td>\n",
       "      <td>0.603297</td>\n",
       "      <td>-0.321621</td>\n",
       "      <td>-0.315332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009039</td>\n",
       "      <td>0.004844</td>\n",
       "      <td>-0.539582</td>\n",
       "      <td>-0.524089</td>\n",
       "      <td>-0.734563</td>\n",
       "      <td>-0.723940</td>\n",
       "      <td>-0.544145</td>\n",
       "      <td>-0.530660</td>\n",
       "      <td>0.599232</td>\n",
       "      <td>0.607080</td>\n",
       "      <td>-0.319187</td>\n",
       "      <td>-0.317532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008528</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>-0.509989</td>\n",
       "      <td>-0.531546</td>\n",
       "      <td>-0.714135</td>\n",
       "      <td>-0.729072</td>\n",
       "      <td>-0.522755</td>\n",
       "      <td>-0.536714</td>\n",
       "      <td>0.616812</td>\n",
       "      <td>0.602606</td>\n",
       "      <td>-0.313360</td>\n",
       "      <td>-0.319989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.009254</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>-0.538532</td>\n",
       "      <td>-0.525259</td>\n",
       "      <td>-0.733847</td>\n",
       "      <td>-0.724748</td>\n",
       "      <td>-0.536827</td>\n",
       "      <td>-0.532926</td>\n",
       "      <td>0.592940</td>\n",
       "      <td>0.607928</td>\n",
       "      <td>-0.322003</td>\n",
       "      <td>-0.316455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008280</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>-1.255748</td>\n",
       "      <td>-0.523310</td>\n",
       "      <td>-1.120602</td>\n",
       "      <td>-0.723402</td>\n",
       "      <td>-0.536979</td>\n",
       "      <td>-0.529657</td>\n",
       "      <td>0.014994</td>\n",
       "      <td>0.612855</td>\n",
       "      <td>-0.320155</td>\n",
       "      <td>-0.315432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_mean_squared_error  \\\n",
       "0  0.030982    0.005196                    -0.549963   \n",
       "1  0.009039    0.004844                    -0.539582   \n",
       "2  0.008528    0.004808                    -0.509989   \n",
       "3  0.009254    0.004604                    -0.538532   \n",
       "4  0.008280    0.004672                    -1.255748   \n",
       "\n",
       "   train_neg_mean_squared_error  test_neg_root_mean_squared_error  \\\n",
       "0                     -0.523218                         -0.741595   \n",
       "1                     -0.524089                         -0.734563   \n",
       "2                     -0.531546                         -0.714135   \n",
       "3                     -0.525259                         -0.733847   \n",
       "4                     -0.523310                         -1.120602   \n",
       "\n",
       "   train_neg_root_mean_squared_error  test_neg_mean_absolute_error  \\\n",
       "0                          -0.723338                     -0.539838   \n",
       "1                          -0.723940                     -0.544145   \n",
       "2                          -0.729072                     -0.522755   \n",
       "3                          -0.724748                     -0.536827   \n",
       "4                          -0.723402                     -0.536979   \n",
       "\n",
       "   train_neg_mean_absolute_error   test_r2  train_r2  \\\n",
       "0                      -0.529527  0.608633  0.603297   \n",
       "1                      -0.530660  0.599232  0.607080   \n",
       "2                      -0.536714  0.616812  0.602606   \n",
       "3                      -0.532926  0.592940  0.607928   \n",
       "4                      -0.529657  0.014994  0.612855   \n",
       "\n",
       "   test_neg_mean_absolute_percentage_error  \\\n",
       "0                                -0.321621   \n",
       "1                                -0.319187   \n",
       "2                                -0.313360   \n",
       "3                                -0.322003   \n",
       "4                                -0.320155   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  \n",
       "0                                 -0.315332  \n",
       "1                                 -0.317532  \n",
       "2                                 -0.319989  \n",
       "3                                 -0.316455  \n",
       "4                                 -0.315432  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores for Random Forest model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_mean_squared_error</th>\n",
       "      <th>train_neg_mean_squared_error</th>\n",
       "      <th>test_neg_root_mean_squared_error</th>\n",
       "      <th>train_neg_root_mean_squared_error</th>\n",
       "      <th>test_neg_mean_absolute_error</th>\n",
       "      <th>train_neg_mean_absolute_error</th>\n",
       "      <th>test_r2</th>\n",
       "      <th>train_r2</th>\n",
       "      <th>test_neg_mean_absolute_percentage_error</th>\n",
       "      <th>train_neg_mean_absolute_percentage_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.934340</td>\n",
       "      <td>0.109862</td>\n",
       "      <td>-0.259727</td>\n",
       "      <td>-0.038768</td>\n",
       "      <td>-0.509635</td>\n",
       "      <td>-0.196896</td>\n",
       "      <td>-0.338179</td>\n",
       "      <td>-0.128448</td>\n",
       "      <td>0.815172</td>\n",
       "      <td>0.970606</td>\n",
       "      <td>-0.195939</td>\n",
       "      <td>-0.072587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.426715</td>\n",
       "      <td>0.667034</td>\n",
       "      <td>-0.274622</td>\n",
       "      <td>-0.036808</td>\n",
       "      <td>-0.524044</td>\n",
       "      <td>-0.191854</td>\n",
       "      <td>-0.340094</td>\n",
       "      <td>-0.124497</td>\n",
       "      <td>0.796028</td>\n",
       "      <td>0.972404</td>\n",
       "      <td>-0.184758</td>\n",
       "      <td>-0.070853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.168864</td>\n",
       "      <td>0.325339</td>\n",
       "      <td>-0.254571</td>\n",
       "      <td>-0.037848</td>\n",
       "      <td>-0.504550</td>\n",
       "      <td>-0.194546</td>\n",
       "      <td>-0.331196</td>\n",
       "      <td>-0.127234</td>\n",
       "      <td>0.808724</td>\n",
       "      <td>0.971704</td>\n",
       "      <td>-0.187935</td>\n",
       "      <td>-0.071748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.407315</td>\n",
       "      <td>0.162041</td>\n",
       "      <td>-0.286175</td>\n",
       "      <td>-0.036967</td>\n",
       "      <td>-0.534953</td>\n",
       "      <td>-0.192269</td>\n",
       "      <td>-0.348132</td>\n",
       "      <td>-0.125647</td>\n",
       "      <td>0.783689</td>\n",
       "      <td>0.972406</td>\n",
       "      <td>-0.196815</td>\n",
       "      <td>-0.070603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.863842</td>\n",
       "      <td>0.101884</td>\n",
       "      <td>-0.273399</td>\n",
       "      <td>-0.037188</td>\n",
       "      <td>-0.522876</td>\n",
       "      <td>-0.192843</td>\n",
       "      <td>-0.339102</td>\n",
       "      <td>-0.125736</td>\n",
       "      <td>0.785546</td>\n",
       "      <td>0.972488</td>\n",
       "      <td>-0.193769</td>\n",
       "      <td>-0.071030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    fit_time  score_time  test_neg_mean_squared_error  \\\n",
       "0   8.934340    0.109862                    -0.259727   \n",
       "1  10.426715    0.667034                    -0.274622   \n",
       "2  13.168864    0.325339                    -0.254571   \n",
       "3  12.407315    0.162041                    -0.286175   \n",
       "4   8.863842    0.101884                    -0.273399   \n",
       "\n",
       "   train_neg_mean_squared_error  test_neg_root_mean_squared_error  \\\n",
       "0                     -0.038768                         -0.509635   \n",
       "1                     -0.036808                         -0.524044   \n",
       "2                     -0.037848                         -0.504550   \n",
       "3                     -0.036967                         -0.534953   \n",
       "4                     -0.037188                         -0.522876   \n",
       "\n",
       "   train_neg_root_mean_squared_error  test_neg_mean_absolute_error  \\\n",
       "0                          -0.196896                     -0.338179   \n",
       "1                          -0.191854                     -0.340094   \n",
       "2                          -0.194546                     -0.331196   \n",
       "3                          -0.192269                     -0.348132   \n",
       "4                          -0.192843                     -0.339102   \n",
       "\n",
       "   train_neg_mean_absolute_error   test_r2  train_r2  \\\n",
       "0                      -0.128448  0.815172  0.970606   \n",
       "1                      -0.124497  0.796028  0.972404   \n",
       "2                      -0.127234  0.808724  0.971704   \n",
       "3                      -0.125647  0.783689  0.972406   \n",
       "4                      -0.125736  0.785546  0.972488   \n",
       "\n",
       "   test_neg_mean_absolute_percentage_error  \\\n",
       "0                                -0.195939   \n",
       "1                                -0.184758   \n",
       "2                                -0.187935   \n",
       "3                                -0.196815   \n",
       "4                                -0.193769   \n",
       "\n",
       "   train_neg_mean_absolute_percentage_error  \n",
       "0                                 -0.072587  \n",
       "1                                 -0.070853  \n",
       "2                                 -0.071748  \n",
       "3                                 -0.070603  \n",
       "4                                 -0.071030  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for model in models:\n",
    "    pipe = make_pipeline(StandardScaler(), models[model])\n",
    "    print(f\"Scores for {model} model.\")\n",
    "    display(pd.DataFrame(\n",
    "        cross_validate(\n",
    "            pipe,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            return_train_score = True,\n",
    "            scoring = score_types_reg\n",
    "        )\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  The Ridge model performs similar to baseline while the Random Forest model performs better as seen with the lower mean squared error scores compared to both the baseline and ridge model.\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Hyperparameter optimization \n",
    "rubric={points:1}\n",
    "\n",
    "1. Carry out hyperparameter optimization using `RandomizedSearchCV` and `Ridge` with the following `param_dist`. The `alpha` hyperparameter of `Ridge` controls the fundamental tradeoff. Choose `neg_mean_absolute_percentage_error` as the HParam optimization metric.\n",
    "\n",
    "2. What was the best `alpha` hyper-parameter found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import loguniform\n",
    "\n",
    "param_dist = {\"ridge__alpha\": loguniform(1e-3, 1e3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. The best hyper-parameter value found was:\n",
      "{'ridge__alpha': 50.430675996731686}\n"
     ]
    }
   ],
   "source": [
    "pipe_ridge = make_pipeline(preprocessor, Ridge())\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipe_ridge, param_dist, cv = 5, n_jobs=-1, return_train_score = True, scoring = \"neg_mean_absolute_percentage_error\"\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"2. The best hyper-parameter value found was:\" )\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Test results\n",
    "rubric={points:4}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Test the best model (from 3.4) on the test set based on the `neg_mean_absolute_percentage_error` score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3162060097627598\n"
     ]
    }
   ],
   "source": [
    "score = random_search.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Model interpretation  \n",
    "rubric={points:4}\n",
    "\n",
    "Ridge is a linear model and it learns coefficients associated with each feature during `fit()`. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore coefficients learned by the `Ridge` model above as a pandas dataframe with two columns: \n",
    "   - features \n",
    "   - coefficients\n",
    "2. Increasing which feature values would result in higher housing price? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MedInc</td>\n",
       "      <td>0.833012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AveBedrms</td>\n",
       "      <td>0.301027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HouseAge</td>\n",
       "      <td>0.121811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Population</td>\n",
       "      <td>-0.001771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AveOccup</td>\n",
       "      <td>-0.042208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AveRooms</td>\n",
       "      <td>-0.266187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Longitude</td>\n",
       "      <td>-0.825614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Latitude</td>\n",
       "      <td>-0.856426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     features  coefficients\n",
       "0      MedInc      0.833012\n",
       "3   AveBedrms      0.301027\n",
       "1    HouseAge      0.121811\n",
       "4  Population     -0.001771\n",
       "5    AveOccup     -0.042208\n",
       "2    AveRooms     -0.266187\n",
       "7   Longitude     -0.825614\n",
       "6    Latitude     -0.856426"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_tuned = make_pipeline(preprocessor, Ridge(alpha=49.33124076676083))\n",
    "ridge_tuned.fit(X_train, y_train)\n",
    "ridge_preds = ridge_tuned.predict(X_test)\n",
    "ridge_preds[:10]\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data={\n",
    "        \"features\": X_train.columns,\n",
    "        \"coefficients\": ridge_tuned.named_steps[\"ridge\"].coef_,\n",
    "    }\n",
    ")\n",
    "\n",
    "df.sort_values(\"coefficients\",ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Increasing these features would result in higher housing prices. MedInc, AveBedrms, HouseAge \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Submission instructions \n",
    "\n",
    "**PLEASE READ:** When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "name": "_merged",
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "438px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
